{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_absolute_error\n",
    "\n",
    "from analytics.machine_learning.price_prediction_with_fundamentals import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "train_set, test_set = utils.split_data_to_train_and_test(\n",
    "    df=dataset,\n",
    "    cutoff_date=dt.datetime(2023,6,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set[['price', 'sector']]\n",
    "X_train = train_set.drop(['price'], axis=1)\n",
    "\n",
    "y_test = test_set[['price', 'sector']]\n",
    "X_test = test_set.drop(['price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    MinMaxScaler\n",
    ")\n",
    "\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_transformed = utils.transform_input(\n",
    "    X=X_train,\n",
    "    one_hot_encoder=one_hot_encoder,\n",
    "    scaler=scaler,\n",
    "    fit=True\n",
    ")\n",
    "\n",
    "X_test_transformed = utils.transform_input(\n",
    "    X=X_test,\n",
    "    one_hot_encoder=one_hot_encoder,\n",
    "    scaler=scaler,\n",
    "    fit=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1686/1686 [==============================] - 1s 623us/step - loss: 50.3641 - val_loss: 53.0035\n",
      "Epoch 2/100\n",
      "1686/1686 [==============================] - 1s 560us/step - loss: 49.2187 - val_loss: 53.3269\n",
      "Epoch 3/100\n",
      "1686/1686 [==============================] - 1s 570us/step - loss: 48.9024 - val_loss: 51.7828\n",
      "Epoch 4/100\n",
      "1686/1686 [==============================] - 1s 552us/step - loss: 48.6864 - val_loss: 51.2192\n",
      "Epoch 5/100\n",
      "1686/1686 [==============================] - 1s 588us/step - loss: 48.4000 - val_loss: 51.2604\n",
      "Epoch 6/100\n",
      "1686/1686 [==============================] - 1s 554us/step - loss: 48.1057 - val_loss: 50.5191\n",
      "Epoch 7/100\n",
      "1686/1686 [==============================] - 1s 569us/step - loss: 47.7139 - val_loss: 51.4771\n",
      "Epoch 8/100\n",
      "1686/1686 [==============================] - 1s 554us/step - loss: 47.4236 - val_loss: 52.3624\n",
      "Epoch 9/100\n",
      "1686/1686 [==============================] - 1s 563us/step - loss: 47.2065 - val_loss: 51.9938\n",
      "Epoch 10/100\n",
      "1686/1686 [==============================] - 1s 616us/step - loss: 47.0696 - val_loss: 54.6653\n",
      "Epoch 11/100\n",
      "1686/1686 [==============================] - 1s 555us/step - loss: 46.9016 - val_loss: 51.9921\n",
      "Epoch 12/100\n",
      "1686/1686 [==============================] - 1s 550us/step - loss: 46.7843 - val_loss: 50.8750\n",
      "Epoch 13/100\n",
      "1686/1686 [==============================] - 1s 555us/step - loss: 46.7268 - val_loss: 50.8869\n",
      "Epoch 14/100\n",
      "1686/1686 [==============================] - 1s 555us/step - loss: 46.6399 - val_loss: 52.9892\n",
      "Epoch 15/100\n",
      "1686/1686 [==============================] - 1s 556us/step - loss: 46.5529 - val_loss: 51.3124\n",
      "Epoch 16/100\n",
      "1686/1686 [==============================] - 1s 623us/step - loss: 46.4282 - val_loss: 52.9284\n",
      "Epoch 17/100\n",
      "1686/1686 [==============================] - 1s 562us/step - loss: 46.3678 - val_loss: 51.7722\n",
      "Epoch 18/100\n",
      "1686/1686 [==============================] - 1s 591us/step - loss: 46.2699 - val_loss: 52.1383\n",
      "Epoch 19/100\n",
      "1686/1686 [==============================] - 1s 568us/step - loss: 46.2237 - val_loss: 52.3459\n",
      "Epoch 20/100\n",
      "1686/1686 [==============================] - 1s 560us/step - loss: 46.1319 - val_loss: 51.6604\n",
      "Epoch 21/100\n",
      "1686/1686 [==============================] - 1s 554us/step - loss: 46.0450 - val_loss: 51.3498\n",
      "Epoch 22/100\n",
      "1686/1686 [==============================] - 1s 569us/step - loss: 46.0033 - val_loss: 52.6247\n",
      "Epoch 23/100\n",
      "1686/1686 [==============================] - 1s 552us/step - loss: 46.0000 - val_loss: 51.7888\n",
      "Epoch 24/100\n",
      "1686/1686 [==============================] - 1s 570us/step - loss: 45.8704 - val_loss: 52.7015\n",
      "Epoch 25/100\n",
      "1686/1686 [==============================] - 1s 602us/step - loss: 45.8255 - val_loss: 53.1308\n",
      "Epoch 26/100\n",
      "1686/1686 [==============================] - 1s 554us/step - loss: 45.7035 - val_loss: 53.8139\n",
      "Epoch 27/100\n",
      "1686/1686 [==============================] - 1s 554us/step - loss: 45.7012 - val_loss: 53.8127\n",
      "Epoch 28/100\n",
      "1686/1686 [==============================] - 1s 553us/step - loss: 45.6861 - val_loss: 54.2766\n",
      "Epoch 29/100\n",
      "1686/1686 [==============================] - 1s 553us/step - loss: 45.6269 - val_loss: 53.5484\n",
      "Epoch 30/100\n",
      "1686/1686 [==============================] - 1s 570us/step - loss: 45.5824 - val_loss: 54.3557\n",
      "Epoch 31/100\n",
      "1686/1686 [==============================] - 1s 551us/step - loss: 45.4495 - val_loss: 55.2816\n",
      "Epoch 32/100\n",
      "1686/1686 [==============================] - 1s 554us/step - loss: 45.4104 - val_loss: 55.4021\n",
      "Epoch 33/100\n",
      "1686/1686 [==============================] - 1s 575us/step - loss: 45.4204 - val_loss: 56.6450\n",
      "Epoch 34/100\n",
      "1686/1686 [==============================] - 1s 574us/step - loss: 45.3452 - val_loss: 55.2829\n",
      "Epoch 35/100\n",
      "1686/1686 [==============================] - 1s 565us/step - loss: 45.2774 - val_loss: 56.4766\n",
      "Epoch 36/100\n",
      "1686/1686 [==============================] - 1s 552us/step - loss: 45.2320 - val_loss: 56.2858\n",
      "Epoch 37/100\n",
      "1686/1686 [==============================] - 1s 552us/step - loss: 45.2142 - val_loss: 57.1716\n",
      "Epoch 38/100\n",
      "1686/1686 [==============================] - 1s 568us/step - loss: 45.1187 - val_loss: 56.7701\n",
      "Epoch 39/100\n",
      "1686/1686 [==============================] - 1s 550us/step - loss: 45.0703 - val_loss: 57.1351\n",
      "Epoch 40/100\n",
      "1686/1686 [==============================] - 1s 598us/step - loss: 44.9585 - val_loss: 56.2744\n",
      "Epoch 41/100\n",
      "1686/1686 [==============================] - 1s 572us/step - loss: 44.9761 - val_loss: 56.8560\n",
      "Epoch 42/100\n",
      "1686/1686 [==============================] - 1s 568us/step - loss: 44.8875 - val_loss: 58.1168\n",
      "Epoch 43/100\n",
      "1686/1686 [==============================] - 1s 570us/step - loss: 44.8314 - val_loss: 58.3028\n",
      "Epoch 44/100\n",
      "1686/1686 [==============================] - 1s 552us/step - loss: 44.7767 - val_loss: 58.2420\n",
      "Epoch 45/100\n",
      "1686/1686 [==============================] - 1s 552us/step - loss: 44.6725 - val_loss: 58.6896\n",
      "Epoch 46/100\n",
      "1686/1686 [==============================] - 1s 550us/step - loss: 44.6180 - val_loss: 58.5198\n",
      "Epoch 47/100\n",
      "1686/1686 [==============================] - 1s 548us/step - loss: 44.4638 - val_loss: 58.2531\n",
      "Epoch 48/100\n",
      "1686/1686 [==============================] - 1s 589us/step - loss: 44.4598 - val_loss: 58.0871\n",
      "Epoch 49/100\n",
      "1686/1686 [==============================] - 1s 565us/step - loss: 44.3673 - val_loss: 58.6266\n",
      "Epoch 50/100\n",
      "1686/1686 [==============================] - 1s 549us/step - loss: 44.2413 - val_loss: 59.2879\n",
      "Epoch 51/100\n",
      "1686/1686 [==============================] - 1s 552us/step - loss: 44.1778 - val_loss: 58.7359\n",
      "Epoch 52/100\n",
      "1686/1686 [==============================] - 1s 556us/step - loss: 44.1962 - val_loss: 58.1398\n",
      "Epoch 53/100\n",
      "1686/1686 [==============================] - 1s 552us/step - loss: 44.0944 - val_loss: 58.5167\n",
      "Epoch 54/100\n",
      "1686/1686 [==============================] - 1s 634us/step - loss: 44.0516 - val_loss: 58.7879\n",
      "Epoch 55/100\n",
      "1686/1686 [==============================] - 1s 571us/step - loss: 44.0452 - val_loss: 58.2729\n",
      "Epoch 56/100\n",
      "1686/1686 [==============================] - 1s 564us/step - loss: 44.0379 - val_loss: 59.7420\n",
      "Epoch 57/100\n",
      "1686/1686 [==============================] - 1s 555us/step - loss: 43.9733 - val_loss: 59.6460\n",
      "Epoch 58/100\n",
      "1686/1686 [==============================] - 1s 565us/step - loss: 43.8754 - val_loss: 58.4144\n",
      "Epoch 59/100\n",
      "1686/1686 [==============================] - 1s 549us/step - loss: 43.8305 - val_loss: 59.3268\n",
      "Epoch 60/100\n",
      "1686/1686 [==============================] - 1s 562us/step - loss: 43.8457 - val_loss: 59.8968\n",
      "Epoch 61/100\n",
      "1686/1686 [==============================] - 1s 550us/step - loss: 43.7337 - val_loss: 58.6495\n",
      "Epoch 62/100\n",
      "1686/1686 [==============================] - 1s 575us/step - loss: 43.7409 - val_loss: 59.1308\n",
      "Epoch 63/100\n",
      "1686/1686 [==============================] - 1s 612us/step - loss: 43.6723 - val_loss: 58.7215\n",
      "Epoch 64/100\n",
      "1686/1686 [==============================] - 1s 567us/step - loss: 43.6687 - val_loss: 57.6146\n",
      "Epoch 65/100\n",
      "1686/1686 [==============================] - 1s 556us/step - loss: 43.5870 - val_loss: 58.9712\n",
      "Epoch 66/100\n",
      "1686/1686 [==============================] - 1s 550us/step - loss: 43.5744 - val_loss: 59.3133\n",
      "Epoch 67/100\n",
      "1686/1686 [==============================] - 1s 548us/step - loss: 43.5106 - val_loss: 58.7570\n",
      "Epoch 68/100\n",
      "1686/1686 [==============================] - 1s 573us/step - loss: 43.4535 - val_loss: 58.9291\n",
      "Epoch 69/100\n",
      "1686/1686 [==============================] - 1s 545us/step - loss: 43.4079 - val_loss: 59.2095\n",
      "Epoch 70/100\n",
      "1686/1686 [==============================] - 1s 568us/step - loss: 43.3758 - val_loss: 58.9912\n",
      "Epoch 71/100\n",
      "1686/1686 [==============================] - 1s 544us/step - loss: 43.3319 - val_loss: 59.0622\n",
      "Epoch 72/100\n",
      "1686/1686 [==============================] - 1s 556us/step - loss: 43.3088 - val_loss: 59.7968\n",
      "Epoch 73/100\n",
      "1686/1686 [==============================] - 1s 549us/step - loss: 43.2244 - val_loss: 58.9676\n",
      "Epoch 74/100\n",
      "1686/1686 [==============================] - 1s 570us/step - loss: 43.2394 - val_loss: 58.9778\n",
      "Epoch 75/100\n",
      "1686/1686 [==============================] - 1s 563us/step - loss: 43.1870 - val_loss: 59.3357\n",
      "Epoch 76/100\n",
      "1686/1686 [==============================] - 1s 544us/step - loss: 43.1865 - val_loss: 59.3269\n",
      "Epoch 77/100\n",
      "1686/1686 [==============================] - 1s 546us/step - loss: 43.1761 - val_loss: 59.4845\n",
      "Epoch 78/100\n",
      "1686/1686 [==============================] - 1s 545us/step - loss: 43.1350 - val_loss: 59.6569\n",
      "Epoch 79/100\n",
      "1686/1686 [==============================] - 1s 547us/step - loss: 43.1034 - val_loss: 59.9186\n",
      "Epoch 80/100\n",
      "1686/1686 [==============================] - 1s 572us/step - loss: 43.0456 - val_loss: 59.2959\n",
      "Epoch 81/100\n",
      "1686/1686 [==============================] - 1s 576us/step - loss: 43.0339 - val_loss: 59.0393\n",
      "Epoch 82/100\n",
      "1686/1686 [==============================] - 1s 549us/step - loss: 42.9687 - val_loss: 59.3516\n",
      "Epoch 83/100\n",
      "1686/1686 [==============================] - 1s 545us/step - loss: 42.9420 - val_loss: 58.5657\n",
      "Epoch 84/100\n",
      "1686/1686 [==============================] - 1s 544us/step - loss: 42.8891 - val_loss: 59.1527\n",
      "Epoch 85/100\n",
      "1686/1686 [==============================] - 1s 572us/step - loss: 42.9241 - val_loss: 58.6740\n",
      "Epoch 86/100\n",
      "1686/1686 [==============================] - 1s 556us/step - loss: 42.9599 - val_loss: 58.6974\n",
      "Epoch 87/100\n",
      "1686/1686 [==============================] - 1s 595us/step - loss: 42.9226 - val_loss: 58.6836\n",
      "Epoch 88/100\n",
      "1686/1686 [==============================] - 1s 546us/step - loss: 42.8660 - val_loss: 59.5722\n",
      "Epoch 89/100\n",
      "1686/1686 [==============================] - 1s 573us/step - loss: 42.8612 - val_loss: 58.8652\n",
      "Epoch 90/100\n",
      "1686/1686 [==============================] - 1s 546us/step - loss: 42.8538 - val_loss: 59.3986\n",
      "Epoch 91/100\n",
      "1686/1686 [==============================] - 1s 571us/step - loss: 42.6706 - val_loss: 59.0774\n",
      "Epoch 92/100\n",
      "1686/1686 [==============================] - 1s 540us/step - loss: 42.7991 - val_loss: 58.6541\n",
      "Epoch 93/100\n",
      "1686/1686 [==============================] - 1s 543us/step - loss: 42.7011 - val_loss: 58.2959\n",
      "Epoch 94/100\n",
      "1686/1686 [==============================] - 1s 559us/step - loss: 42.6876 - val_loss: 58.7278\n",
      "Epoch 95/100\n",
      "1686/1686 [==============================] - 1s 548us/step - loss: 42.7247 - val_loss: 57.5078\n",
      "Epoch 96/100\n",
      "1686/1686 [==============================] - 1s 543us/step - loss: 42.6965 - val_loss: 58.8061\n",
      "Epoch 97/100\n",
      "1686/1686 [==============================] - 1s 576us/step - loss: 42.7142 - val_loss: 57.8950\n",
      "Epoch 98/100\n",
      "1686/1686 [==============================] - 1s 545us/step - loss: 42.6797 - val_loss: 57.6561\n",
      "Epoch 99/100\n",
      "1686/1686 [==============================] - 1s 546us/step - loss: 42.6647 - val_loss: 58.2230\n",
      "Epoch 100/100\n",
      "1686/1686 [==============================] - 1s 558us/step - loss: 42.5845 - val_loss: 58.6158\n",
      "99/99 [==============================] - 0s 386us/step - loss: 58.6158\n",
      "Test Loss: 58.615806579589844\n",
      "99/99 [==============================] - 0s 378us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orestis/MyProjects/InvestorAPI/env/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Build the neural network model\n",
    "model = keras.Sequential([\n",
    "    # Input layer\n",
    "    keras.layers.Input(shape=(100,)),  # Adjust the input shape according to your data\n",
    "    \n",
    "    # Hidden layers\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    \n",
    "    # Output layer (a single neuron for regression)\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')  # You can also use 'mean_absolute_error' for MAE\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_transformed, y_train['price'], epochs=100, batch_size=32, validation_data=(X_test_transformed, y_test['price']))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test_transformed, y_test['price'])\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "# You can also make predictions using the trained model\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "# Save and load the model (optional)\n",
    "model.save(\"regression_model.h5\")\n",
    "# To load the model later: loaded_model = keras.models.load_model(\"regression_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 58.62\n",
      "Coefficient of determination: -0.11\n",
      "Mean absolute pct error: 0.86\n"
     ]
    }
   ],
   "source": [
    "# The mean squared error\n",
    "print(\"Mean absolute error: %.2f\" % mean_absolute_error(y_test['price'], y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test['price'], y_pred))\n",
    "print(\"Mean absolute pct error: %.2f\" % mean_absolute_percentage_error(y_test['price'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to improve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "843/843 [==============================] - 2s 2ms/step - loss: 21882.2012 - val_loss: 32248.4531\n",
      "Epoch 2/100\n",
      "843/843 [==============================] - 1s 1ms/step - loss: 21389.5273 - val_loss: 28528.8496\n",
      "Epoch 3/100\n",
      "843/843 [==============================] - 1s 1ms/step - loss: 21050.5742 - val_loss: 47492.0117\n",
      "Epoch 4/100\n",
      "843/843 [==============================] - 1s 1ms/step - loss: 20676.7734 - val_loss: 63153.0352\n",
      "Epoch 5/100\n",
      "843/843 [==============================] - 1s 1ms/step - loss: 20046.1348 - val_loss: 114784.3750\n",
      "Epoch 6/100\n",
      "843/843 [==============================] - 1s 1ms/step - loss: 19804.8203 - val_loss: 74047.6016\n",
      "Epoch 7/100\n",
      "843/843 [==============================] - 1s 1ms/step - loss: 19668.3086 - val_loss: 570282.5625\n",
      "Epoch 8/100\n",
      "843/843 [==============================] - 1s 1ms/step - loss: 19559.7656 - val_loss: 4082599.5000\n",
      "Epoch 9/100\n",
      "843/843 [==============================] - 1s 1ms/step - loss: 19517.3242 - val_loss: 924453.1875\n",
      "Epoch 10/100\n",
      "843/843 [==============================] - 1s 1ms/step - loss: 19463.3379 - val_loss: 1134111.2500\n",
      "Epoch 11/100\n",
      "843/843 [==============================] - 1s 1ms/step - loss: 19431.3711 - val_loss: 913537.0625\n",
      "Epoch 12/100\n",
      "843/843 [==============================] - 1s 1ms/step - loss: 19232.6621 - val_loss: 183780.3906\n",
      "99/99 [==============================] - 0s 420us/step\n",
      "Test Mean Squared Error: 28496.63\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Build a more complex neural network\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(100,)),\n",
    "    keras.layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Implement learning rate scheduling\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2, decay_steps=10000, decay_rate=0.9)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Implement early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_transformed,\n",
    "    y_train['price'],\n",
    "    epochs=100, batch_size=64,\n",
    "    validation_data=(X_test_transformed, y_test['price']),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "test_loss = mean_squared_error(y_test['price'], y_pred)\n",
    "print(f\"Test Mean Squared Error: {test_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 57.53\n",
      "Coefficient of determination: -0.01\n",
      "Mean absolute pct error: 3.49\n"
     ]
    }
   ],
   "source": [
    "# The mean squared error\n",
    "print(\"Mean absolute error: %.2f\" % mean_absolute_error(y_test['price'], y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test['price'], y_pred))\n",
    "print(\"Mean absolute pct error: %.2f\" % mean_absolute_percentage_error(y_test['price'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
